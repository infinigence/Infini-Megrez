<div align="center">
  <img src="./assets/megrez-logo.png" alt="Megrez Logo" width="400" />

  <br>

  <a href="https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-Preview">
    <b>ğŸ¤— Hugging Face</b>
  </a> &nbsp;|&nbsp;
  <a href="https://www.modelscope.cn/models/InfiniAI/Megrez2-3x7B-A3B-Preview">
    <b>ğŸ¤– Model Scope</b>
  </a> &nbsp;|&nbsp;
  <a href="./docs/tech_report.pdf">
    <b>ğŸ“„ Tech Report</b>
  </a> &nbsp;|&nbsp;
  <a href="https://huggingface.co/spaces/Infinigence/Megrez2-3x7B-A3B-Preview">
    <b>ğŸ’» Demo</b>
  </a> &nbsp;|&nbsp;
  <a href="./assets/wechat-official.jpg">
    <b>ğŸ’¬ WeChat Official</b>
  </a> &nbsp;

  <br>

  <strong>ä¸­æ–‡ | [English](./README_EN.md)</strong>

</div>

# æ›´æ–°æ—¥å¿—

- [2025.07.24] å‘å¸ƒ [Megrez2-3x7B-A3B-Preview](https://github.com/infinigence/Infini-Megrez/tree/main) ä¸“ä¸ºç»ˆç«¯è®¾å¤‡è®¾è®¡çš„å¤§æ¨¡å‹ï¼Œå…¼é¡¾MoEçš„ç²¾åº¦æ æ†ä¸Denseçš„æ€»å‚æ•°é‡å‹å¥½ã€‚

- [2024.12.16] å‘å¸ƒ [Megrez-3B-Omni](https://huggingface.co/Infinigence/Megrez-3B-Omni) åŸºäºMegrez-3B-Instruct æ‰©å±•ï¼ŒåŒæ—¶å…·å¤‡å›¾ç‰‡ã€æ–‡æœ¬ã€éŸ³é¢‘ä¸‰ç§æ¨¡æ€æ•°æ®çš„ç†è§£åˆ†æèƒ½åŠ›ã€‚

- [2024.12.16] å‘å¸ƒ [Megrez-3B-Instruct](https://github.com/infinigence/Infini-Megrez/tree/Megrez-3B) æ¨¡å‹èƒ½å¤Ÿæ¯”è‚© Yi-1.5-6B-Chatã€ Qwen2-7B-Instructã€ GLM-4-9B-Chat å’Œ Baichuan2-13B-Chat ç­‰å¤šä¸ª6B-13Bå‚æ•°é‡çš„æ¨¡å‹ã€‚

# æ¨¡å‹ä¸‹è½½

<div align="center">

| HuggingFace | ModelScope | Wisemodel |
|:---:|:---:|:---:|
| [Megrez2-3x7B-A3B-Preview](https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-Preview) | [Megrez2-3x7B-A3B-Preview](https://www.modelscope.cn/models/InfiniAI/Megrez2-3x7B-A3B-Preview) | [Megrez2-3x7B-A3B-Preview](https://wisemodel.cn/models/Infinigence/Megrez2-3x7B-A3B-Preview) |
| [Megrez-3B-Omni](https://huggingface.co/Infinigence/Megrez-3B-Omni) | [Megrez-3B-Omni](https://www.modelscope.cn/models/InfiniAI/Megrez-3B-Omni) | [Megrez-3B-Omni](https://www.wisemodel.cn/models/Infinigence/Megrez-3B-Omni) |
| [Megrez-3B-Instruct](https://huggingface.co/Infinigence/Megrez-3B-Instruct) | [Megrez-3B-Instruct](https://www.modelscope.cn/models/InfiniAI/Megrez-3b-Instruct) | [Megrez-3B-Instruct](https://www.wisemodel.cn/models/Infinigence/Megrez-3B-Instruct) |

</div>


# Megrez2-3x7B-A3B-Preview

## æ¨¡å‹ç®€ä»‹

Megrez2-3x7B-A3B-Preview æ˜¯ä¸“ä¸ºç»ˆç«¯è®¾å¤‡è®¾è®¡çš„å¤§æ¨¡å‹ï¼Œå…¼é¡¾MoEçš„ç²¾åº¦æ æ†ä¸Denseçš„æ€»å‚æ•°é‡å‹å¥½ã€‚æœ¬æ¬¡å‘å¸ƒçš„ä¸ºMegrez 2.0é¢„è§ˆç‰ˆæœ¬ï¼Œè®­ç»ƒæ•°æ®é‡5T Tokensï¼Œæœªæ¥æˆ‘ä»¬è®¡åˆ’å®Œæˆæ›´å¤§è§„æ¨¡çš„æ•°æ®è®­ç»ƒï¼Œå¹¶æé«˜æ¨¡å‹çš„æ¨ç†å’ŒAgentèƒ½åŠ›ï¼Œæ­£å¼ç‰ˆæœ¬é¢„è®¡ä»Šå¹´å¹´å†…å‘å¸ƒã€‚

## åŸºç¡€ä¿¡æ¯

<div align="center">

| | |
|:---:|:---:|
| **Architecture** | Mixture-of-Experts (MoE) |
| **Total Parameters** | 3x7B |
| **Activated Parameters** | 3B |
| **Experts Shared Frequency**| 3 |
| **Number of Layers** (Dense layer included) | 31 |
| **Number of Dense Layers** | 1 |
| **Attention Hidden Dimension** | 2048 |
| **MoE Hidden Dimension** (per Expert) | 1408 |
| **Number of Attention Heads** | 16 |
| **Number of Experts** | 64 |
| **Selected Experts per Token** | 6 |
| **Number of Shared Experts** | 4 |
| **Vocabulary Size** | 128,880 |
| **Context Length** | 32K |
| **Base Frequency of RoPE** | 1,000,000 |
| **Attention Mechanism** | GQA |
| **Activation Function** | SwiGLU |
</div>

## æ€§èƒ½æµ‹è¯•

æˆ‘ä»¬ä½¿ç”¨å¼€æºè¯„æµ‹å·¥å…· [OpenCompass](https://github.com/open-compass/opencompass) å¯¹ Megrez2-3x7B-A3B-Preview è¿›è¡Œäº†è¯„æµ‹ï¼Œéƒ¨åˆ†è¯„æµ‹ç»“æœå¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚

<div align="center">
<table>
<thead>
<tr>
<th align="center">Benchmark</th>
<th align="center">Metric</th>
<th align="center"><sup>Megrez2-3x7B<br>-A3B-Preview</sup></th>
<th align="center"><sup>Qwen2.5-3B</sup></th>
<th align="center"><sup>Qwen2.5-7B</sup></th>
<th align="center"><sup>Qwen3-4B</sup></th>
<th align="center"><sup>Qwen3-8B</sup></th>
<th align="center"><sup>Phi-4-mini</sup></th>
<th align="center"><sup>Gemma-3-4B</sup></th>
<th align="center"><sup>GPT-4o-mini <br><sup>2024-07-18</sup></sup></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Activate Params (B)</td>
<td align="center"></td>
<td align="center">3.0</td>
<td align="center">3.1</td>
<td align="center">7.6</td>
<td align="center">4.0</td>
<td align="center">8.2</td>
<td align="center">3.8</td>
<td align="center">4.3</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center">Stored Params (B)</td>
<td align="center"></td>
<td align="center">7.5</td>
<td align="center">3.1</td>
<td align="center">7.6</td>
<td align="center">4.0</td>
<td align="center">8.2</td>
<td align="center">3.8</td>
<td align="center">4.3</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center" colspan=9><strong>General Tasks</strong></td>
</tr>
<tr>
<td align="center">C-EVAL</td>
<td align="center">EM</td>
<td align="center"><strong>91.7</strong></td>
<td align="center">68.2</td>
<td align="center">76.2</td>
<td align="center">72.2</td>
<td align="center">77.9</td>
<td align="center">40.0</td>
<td align="center">-</td>
<td align="center">66.3</td>
</tr>
<tr>
<td align="center">MMLU-Pro</td>
<td align="center">EM</td>
<td align="center"><strong>67.6</strong></td>
<td align="center">43.7</td>
<td align="center">56.3</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">52.8</td>
<td align="center">43.6</td>
<td align="center">-</td>
</tr>
<td align="center" colspan=9><strong>Instruction Tasks</strong></td>
<tr>
<td align="center">IF-Eval</td>
<td align="center">Prompt Strict</td>
<td align="center">80.2</td>
<td align="center">58.2</td>
<td align="center">71.2</td>
<td align="center">81.2</td>
<td align="center">83.0</td>
<td align="center">68.6</td>
<td align="center"><strong>90.2</strong></td>
<td align="center">80.4</td>
</tr>
<td align="center" colspan=9><strong>Math & STEM Tasks</strong></td>
<tr>
<td align="center">MATH-500</td>
<td align="center">EM</td>
<td align="center">81.6</td>
<td align="center">65.9</td>
<td align="center">75.5</td>
<td align="center">84.8</td>
<td align="center"><strong>87.4</strong></td>
<td align="center">64.0</td>
<td align="center">75.6</td>
<td align="center">78.2</td>
</tr>
<tr>
<td align="center">GSM8K</td>
<td align="center">EM</td>
<td align="center">83.6</td>
<td align="center">86.7</td>
<td align="center">91.6</td>
<td align="center">-</td>
<td align="center"><strong>93.2</strong></td>
<td align="center">88.6</td>
<td align="center">89.2</td>
<td align="center">-</td>
</tr>
<td align="center" colspan=9><strong>Coding Tasks</strong></td>
<tr>
<td align="center">HumanEval</td>
<td align="center">Pass@1</td>
<td align="center">74.4</td>
<td align="center">74.4</td>
<td align="center">84.8</td>
<td align="center">-</td>
<td align="center"><strong>85.9</strong></td>
<td align="center">74.4</td>
<td align="center">71.3</td>
<td align="center">87.2</td>
</tr>
<tr>
<td align="center">MBPP</td>
<td align="center">Pass@1</td>
<td align="center"><strong>88.0</strong></td>
<td align="center">72.7</td>
<td align="center">79.2</td>
<td align="center">-</td>
<td align="center">77.0</td>
<td align="center">65.3</td>
<td align="center">63.2</td>
<td align="center">-</td>
</tr>
</tbody>
</table>
</div>

## å¦‚ä½•è¿è¡Œ

### Transformers

æ¨èä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„ `transformers` æˆ–è€… `transformers>=4.52.4` çš„ç‰ˆæœ¬ã€‚
ä»¥ä¸‹æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„ä»£ç ç‰‡æ®µç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•è¿è¡Œ Megrez2-3x7B-A3B-Preview æ¨¡å‹ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

path = "Infinigence/Megrez2-3x7B-A3B-Preview"
device = "cuda"

tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch.bfloat16, device_map=device, trust_remote_code=True)

messages = [
    {"role": "user", "content": "ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯å“ªåº§ï¼Ÿ"},
]
model_inputs = tokenizer.apply_chat_template(messages, return_tensors="pt", add_generation_prompt=True).to(device)

model_outputs = model.generate(
    model_inputs,
    do_sample=True,
    max_new_tokens=1024
)

output_token_ids = [
    model_outputs[i][len(model_inputs[i]):] for i in range(len(model_inputs))
]

responses = tokenizer.batch_decode(output_token_ids, skip_special_tokens=True)[0]
print(responses)

# ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯ç ç©†æœ—ç›å³°ï¼ˆMount Everestï¼‰ï¼Œä½äºå–œé©¬æ‹‰é›…å±±è„‰çš„ä¸­å°¼è¾¹å¢ƒã€‚ç ç©†æœ—ç›å³°çš„æµ·æ‹”é«˜åº¦ä¸º8,848.86ç±³ï¼ˆ29,031.7è‹±å°ºï¼‰ï¼Œè¿™ä¸€æ•°æ®æ˜¯ç”±ä¸­å›½å’Œå°¼æ³Šå°”åœ¨2020å¹´å…±åŒå®£å¸ƒçš„æœ€æ–°æµ‹é‡ç»“æœã€‚ç ç©†æœ—ç›å³°ä¸ä»…æ˜¯ç™»å±±çˆ±å¥½è€…çš„åœ£åœ°ï¼Œä¹Ÿæ˜¯åœ°ç†å’Œç§‘å­¦ç ”ç©¶çš„é‡è¦å¯¹è±¡ã€‚
```

### ModelScope

`ModelScope` é‡‡ç”¨äº†ä¸ `Transformers` ç±»ä¼¼ï¼ˆä½†ä¸å®Œå…¨ä¸€è‡´ï¼‰çš„ç¼–ç¨‹æ¥å£ã€‚å¯¹äºåŸºç¡€ä½¿ç”¨ï¼Œä»…éœ€å°†ä¸Šé¢ä»£ç ç¬¬ä¸€è¡Œåšå¦‚ä¸‹ä¿®æ”¹ï¼š

```python
from modelscope import AutoModelForCausalLM, AutoTokenizer
```

### llama.cpp

å³å°†åˆ°æ¥...

## å¦‚ä½•éƒ¨ç½²

### vLLM

æ¨è `vllm>=0.9.2` çš„ç‰ˆæœ¬

#### vLLM ç¦»çº¿
```shell
cd demo/vllm
export MODEL_PATH="Infinigence/Megrez2-3x7B-A3B-Preview"
python3 infer_vllm_offline.py $MODEL_PATH
```
#### vLLM åœ¨çº¿
åœ¨ç»ˆç«¯ä¸­å¯åŠ¨vLLMæœåŠ¡ï¼Œå‘½ä»¤å¦‚ä¸‹
```shell
cd demo/vllm
export MODEL_PATH="Infinigence/Megrez2-3x7B-A3B-Preview"
python3 serve_llm_online.py serve $MODEL_PATH --gpu-memory-utilization 0.9 --served-model-name megrez-moe --trust_remote_code
```

ç°åœ¨ï¼Œå¯ä»¥é€šè¿‡curlå‘é€è¯·æ±‚
```shell
curl --location 'http://localhost:8000/v1/chat/completions' \
--header 'Content-Type: application/json' \
--header 'Authorization: Bearer sk-123456' \
--data '{
    "model": "megrez-moe",
    "messages": [
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯å“ªåº§ï¼Ÿ"
                }
            ]
        }
    ]
}'
```

### SGLang

æ¨è `sglang>=0.4.9.post2` çš„ç‰ˆæœ¬
```shell
cd demo/sglang
export MODEL_PATH="Infinigence/Megrez2-3x7B-A3B-Preview" 
python3 infer_sglang_offline.py $MODEL_PATH
```


## æœ€ä½³å®è·µ

ä¸ºäº†è·å¾—æœ€ä½³æ€§èƒ½ï¼Œå»ºè®®ä»¥ä¸‹è®¾ç½®ï¼š

1. é‡‡æ ·å‚æ•°ï¼šæ¨èä½¿ç”¨ Temperature=0.7 å’Œ TopP=0.9 ã€‚

2. æ ‡å‡†åŒ–è¾“å‡ºæ ¼å¼ï¼šåœ¨åŸºå‡†æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨æç¤ºæ¥æ ‡å‡†åŒ–æ¨¡å‹è¾“å‡ºï¼Œæ¯”å¦‚ï¼š
    * æ•°å­¦é—®é¢˜ï¼šåœ¨æç¤ºä¸­åŒ…å«â€œè¯·é€æ­¥æ¨ç†ï¼Œå¹¶å°†æœ€ç»ˆç­”æ¡ˆæ”¾åœ¨\boxed{}ä¸­ã€‚â€
    * é€‰æ‹©é¢˜ï¼šåœ¨æç¤ºä¸­æ·»åŠ ä»¥ä¸‹ JSON ç»“æ„ä»¥æ ‡å‡†åŒ–å“åº”ï¼šâ€œè¯·åœ¨ answer å­—æ®µä¸­ä»…ä»¥é€‰æ‹©å­—æ¯çš„å½¢å¼æ˜¾ç¤ºæ‚¨çš„é€‰æ‹©ï¼Œä¾‹å¦‚ "answer": "C" ã€‚â€

# Megrez-3B-Omni

è¯¦ç»†ä¿¡æ¯è¯·æŸ¥çœ‹ [Infini-Megrez-Omni ä»“åº“](https://github.com/infinigence/Infini-Megrez-Omni)

# Megrez-3B-Instruct

è¯¦ç»†ä¿¡æ¯è¯·æŸ¥çœ‹ [Megrez-3B åˆ†æ”¯](https://github.com/infinigence/Infini-Megrez/tree/Megrez-3B)

# è®¸å¯å£°æ˜

æˆ‘ä»¬æ‰€æœ‰çš„å¼€æºæ¨¡å‹å‡é‡‡ç”¨Apache 2.0åè®®æˆæƒã€‚

# å¼•ç”¨ä¿¡æ¯

å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹æœ‰ç”¨ï¼Œè¯·å¼•ç”¨ä»¥ä¸‹ä¿¡æ¯ã€‚

```bibtex
@misc{li2025megrez2technicalreport,
      title={Megrez2 Technical Report}, 
      author={Boxun Li and Yadong Li and Zhiyuan Li and Congyi Liu and Weilin Liu and Guowei Niu and Zheyue Tan and Haiyang Xu and Zhuyu Yao and Tao Yuan and Dong Zhou and Yueqing Zhuang and Bo Zhao and Guohao Dai and Yu Wang},
      year={2025},
      eprint={2507.17728},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.17728}, 
}
```

# è”ç³»æˆ‘ä»¬

å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·éšæ—¶æäº¤GitHub issueæˆ–è”ç³»[å¾®ä¿¡ç¾¤ç»„](./assets/wechat-group.jpg)ã€‚